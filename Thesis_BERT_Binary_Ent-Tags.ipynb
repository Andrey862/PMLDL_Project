{"nbformat":4,"nbformat_minor":4,"metadata":{"notebookPath":"Thesis_BERT_Binary_Ent-Tags.ipynb","language_info":{"file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","version":"3.7.7"},"notebookId":"2fc1e7e1-5cc7-4a1d-b2f5-540a30f03d1b","kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"ydsNotebookPath":"Thesis_BERT_Binary_Ent-Tags.ipynb"},"cells":[{"cell_type":"code","source":"#!g1.1\n# !git clone https://github.com/nerel-ds/NEREL","metadata":{"outputId":"2f60b594-2d3f-407b-8b11-6639cd00d2aa","id":"8vRu5JKz1ir8","cellId":"c8aacc5a-0610-479b-af39-c936bb1ff95a","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":161},{"cell_type":"code","source":"#!g1.1\n# !unzip NEREL-v1.0.zip -d NEREL","metadata":{"scrolled":true,"cellId":"1ty6mfv872tle06tj03k2","trusted":true},"outputs":[],"execution_count":162},{"cell_type":"code","source":"#!g1.1\n## Read the data# Reading files\nfrom collections import namedtuple\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport nltk\n\nAnn = namedtuple('annotation', ['tag', 'start1', 'end1', 'start2', 'end2', 'text'])\nRel = namedtuple('relationship', ['tag', 'arg1', 'arg2'])\n\ndef read_files(folder):\n    filenames = sorted(set(e[:e.rfind('.')] for e in os.listdir(folder)))[1:]\n    \n    texts, entities, relationships  = [], [], []\n    for file in tqdm(filenames):\n       # if (not file): continue\n        path1 = os.path.join(folder, file+'.txt')\n        path2 = os.path.join(folder, file+'.ann')\n        if (not os.path.exists(path2)):\n            print(f'{path2} not found')\n            continue\n            #with open(path2, 'w'):\n            #    pass\n            \n        with open(path1, 'r', encoding=\"utf8\") as text, open(path2, 'r', encoding=\"utf8\") as ann:\n            texts.append(text.read())\n\n            file_entities = {}\n            file_relationship = []\n            regex_ent = r'T(?P<id>\\d+)\\s(?P<tag>\\w+)\\s(?P<start1>\\d+) (?P<end1>\\d+)(;(?P<start2>\\d+) (?P<end2>\\d+))?\\s(?P<text>.*)'\n            regex_rel = r'R(?P<id>\\d+)\\s(?P<tag>\\w+)\\sArg1:T(?P<arg1>\\d+) Arg2:T(?P<arg2>\\d+)'\n            \n            \n            for row in sorted(ann.readlines(), reverse=True):\n                #print(row)\n                match_ent = re.match(regex_ent, row)\n                match_rel = re.match(regex_rel, row)\n                if (match_ent):\n                    res = match_ent.groupdict()\n                    res['start1'] = int(res['start1'])\n                    res['end1'] = int(res['end1'])\n                    if (res['start2'] is not None):\n                        res['start2'] = int(res['start2'])\n                        res['end2'] = int(res['end2'])\n                    id = res.pop('id')\n                    file_entities[id] = Ann(**res)\n                elif (match_rel):\n                    try:\n                        res = match_rel.groupdict()\n                        res['arg1'] = file_entities[res['arg1']]\n                        res['arg2'] = file_entities[res['arg2']]\n                        id = res.pop('id')\n                        file_relationship.append(Rel(**res))\n                    except KeyError as e:\n                        print(f'not found T{e} row={row}')\n                else:\n                    print(f'incorrect format in: row={row} file={file}')\n            entities.append(file_entities)\n            relationships.append(file_relationship)\n    entities = [sorted(e.values(), key = lambda x: (x.start1, x.end1)) for e in entities]\n    return texts, entities, relationships, filenames","metadata":{"id":"i0hrNfhb17mT","cellId":"eb41719b-969a-42ca-b86b-a342cdba7d47","trusted":true},"outputs":[],"execution_count":163},{"cell_type":"code","source":"#!g1.1\nfolder = 'NEREL/NEREL-v1.0/train'\ntexts, entities, relationships, filenames = read_files(folder)","metadata":{"outputId":"af7aafa1-8ff4-4710-dc53-25c63fcb19bf","id":"R8DaAwoP18sY","cellId":"fa765711-be27-4ead-adbe-3a89b50071d6","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":"incorrect format in: row=По словам очевидцев пешехо\n file=21013_text\n"},{"output_type":"stream","name":"stderr","text":"100%|██████████| 745/745 [00:01<00:00, 452.36it/s]\n"}],"execution_count":164},{"cell_type":"code","source":"#!g1.1\ndef in_range(range1, range2):\n    # range1 in range2\n    if range1[0] is None: return True\n    if range2[0] is None: return False\n    return int(range2[0]) <= int(range1[0]) and int(range2[1]) >= int(range1[1])\n\ndef is_nested_anns(ent1: Ann, ent2: Ann):\n    # ent 1 in ent 2\n    res = True\n    ent1_range1 = (ent1.start1, ent1.end1)\n    ent1_range2 = (ent1.start2, ent1.end2)\n    ent2_range1 = (ent2.start1, ent2.end1)\n    ent2_range2 = (ent2.start2, ent2.end2)\n    #print(f'{ent1_range1=} {ent1_range2=} {ent1_range2=} {ent2_range2=}')\n    res = res and (in_range(ent1_range1, ent2_range1) or in_range(ent1_range1, ent2_range2))\n    res = res and (in_range(ent1_range2, ent2_range1) or in_range(ent1_range2, ent2_range2))\n    return res\n\n\ndef is_nested_anns2(ent1: Ann, ent2: Ann):\n    return is_nested_anns(ent1,ent2) or is_nested_anns(ent2, ent1)\n\ndef is_nested(rel: Rel):\n    return is_nested_anns(rel.arg1, rel.arg2) or is_nested_anns(rel.arg2, rel.arg1)","metadata":{"id":"h6WVK5qx1-EL","cellId":"2a1fb212-0507-4f22-a453-4aa5d73e56ab","trusted":true},"outputs":[],"execution_count":165},{"cell_type":"markdown","source":"![](https://i.imgur.com/tgDfc8i.png)             | ![](https://i.imgur.com/oWa5vWo.png)\n:-------------------------:|:-------------------------:\n","metadata":{"cellId":"95ec3a08-ff60-40da-a345-8cb8c139c29e","id":"ZQUnNVl22pRD"}},{"cell_type":"code","source":"#!g1.1\n%pip install transformers","metadata":{"outputId":"9ff56f9d-ff91-498d-8d38-0bcbc27555cd","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"UvEar7XL3O_u","scrolled":true,"cellId":"8f3082f8-bbfc-40ce-8e38-3ef6f998f18f"},"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (2.5.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2021.7.6)\nRequirement already satisfied: numpy in /kernel/lib/python3.7/site-packages (from transformers) (1.19.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\nRequirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.50.0)\nRequirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.12.31)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.96)\nRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\nRequirement already satisfied: requests in /kernel/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers) (1.15.49)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers) (0.10.0)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers) (0.3.7)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /kernel/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (2.8.2)\nRequirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.7/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (0.15.2)\nRequirement already satisfied: urllib3<1.26,>=1.20 in /home/jupyter/.local/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (1.25.11)\nRequirement already satisfied: six>=1.5 in /kernel/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.31->boto3->transformers) (1.16.0)\nRequirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\nRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\n"}],"execution_count":166},{"cell_type":"code","source":"#!g1.1\n# pre_dataset = []\n\n# for text_id in range(len(texts)):\n#     relationships_nested = {(e.arg1, e.arg2):e for e in relationships[text_id] if is_nested(e)}\n#     nes = []\n#     for i in range(len(entities[text_id])):\n#         # O(n^2) eeeeeeeeee\n#         for j in range(i+1, len(entities[text_id])):\n#             if (is_nested_anns2(entities[text_id][i], entities[text_id][j])):\n#                 nes.append((entities[text_id][i], entities[text_id][j]))\n#     for e in nes:\n#         if (e in relationships_nested):\n#             pre_dataset.append((*e, relationships_nested[e].tag))\n#         elif ((e[1], e[0]) in relationships_nested):\n#             pre_dataset.append((e[1], e[0], relationships_nested[(e[1], e[0])].tag))\n#         else:\n#             pre_dataset.append((*e, 'None'))\n# #             pre_dataset.append((e[1], e[0], 'None'))","metadata":{"cellId":"i1lssnl9wthtrdx3u011j","trusted":true},"outputs":[],"execution_count":167},{"cell_type":"markdown","source":"# Dataset","metadata":{"cellId":"hbfwitjbqoiti9fghjjy8"}},{"cell_type":"code","source":"#!g1.1\nimport os\nimport torch\nimport time\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport torchvision\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport random\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"id":"niitsyy82pfO","cellId":"fe996418-4911-4f47-9bc5-29970268b416","trusted":true},"outputs":[],"execution_count":168},{"cell_type":"code","source":"#!g1.1\nfrom transformers import BertForSequenceClassification, BertTokenizer","metadata":{"id":"ExNYmaho3aFO","cellId":"cae0d712-a4a9-47f2-8505-788b718f8950","trusted":true},"outputs":[],"execution_count":169},{"cell_type":"code","source":"#!g1.1\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using {} device'.format(device))","metadata":{"outputId":"3ef609a8-69a1-4fc1-868c-5573024aa98d","id":"EP9xO8an3Kyd","cellId":"8f8d67ac-4008-498e-b5fd-444672043c1b","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":"Using cuda device\n"}],"execution_count":170},{"cell_type":"code","source":"#!g1.1\ntokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased-sentence')","metadata":{"outputId":"119cabe3-2ffc-4fd7-9707-b3315dec26dc","id":"bj1_NVlF4SQ5","cellId":"f18b7438-46b2-45f8-ae45-db5ea42e4bf3","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["f77da0d2f01645e39f61678915e1e5dd","15582376f7984de884ebcd1f913fbe57","ffd5f1e21d8e456fa88f1ed81df1d988","d57085df67b64bc89f50e9e201ffd831","4dbb5dbc1e504307babd12634d29c81e","76fd931459754c148c320854fa91797c","58eed6a400664d74a790dd72255fd63b","c3153df30cdf480d8a045d1e079ec8eb","f7bdb939e91f4345b79ef35da1b4362b","47dbf5127c9647d585bc3dcf6a028851","2e92c8deed234326aa88b0b23ae16d44","56be7515ba554ac79d6a2e70e6d272d7","7be91583260c4c9e97827d65d150566b","cd4ef76a141046728258c305dd36315f"]}},"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1649718.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bdf3e4436ae4855b26deb2a0fad512a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=112.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b85c3962acf41db89aeed4c682ce60d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=24.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fbc0bf3909e4863ad7c64d1690bb6f2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"\n\n\n"}],"execution_count":171},{"cell_type":"code","source":"#!g1.1\nMAX_LENGTH = 100","metadata":{"id":"hZsLjKts8LDL","cellId":"34f918fb-625c-4e9c-a80c-353f55d96dab","trusted":true},"outputs":[],"execution_count":172},{"cell_type":"code","source":"#!g1.1\nclass Binary_Tag:\n    def __getitem__(self, x):\n        if (x == 'None'): return 0\n        else: return 1\n\n# tags = list(set(e.tag for e in sum(relationships,[]))) + ['None'] # ids --> string tag\nreversed_tags = Binary_Tag() # string tag --> ids\n\nentity_tags = list(set(e.tag for e in sum(entities,[]))) \n\ndef tok(text1):\n    # longest entity is 36 tokens long\n    res = tokenizer.encode_plus(text1,\n                      max_length = MAX_LENGTH,\n                      pad_to_max_length = True,\n                      return_tensors = 'pt',\n                      )\n    return res['input_ids'][0], res['attention_mask'][0], res['token_type_ids'][0]\n    \nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, folder):\n        texts, entities, relationships, filenames = read_files(folder)\n        \n        pre_dataset = []\n\n        for text_id in range(len(texts)):\n            relationships_nested = {(e.arg1, e.arg2):e for e in relationships[text_id] if is_nested(e)}\n            nes = []\n            for i in range(len(entities[text_id])):\n                # O(n^2) eeeeeeeeee\n                for j in range(i+1, len(entities[text_id])):\n                    if (is_nested_anns2(entities[text_id][i], entities[text_id][j])):\n                        nes.append((entities[text_id][i], entities[text_id][j]))\n            for e in nes:\n                if (e in relationships_nested):\n                    pre_dataset.append((*e, relationships_nested[e].tag))\n                elif ((e[1], e[0]) in relationships_nested):\n                    pre_dataset.append((e[1], e[0], relationships_nested[(e[1], e[0])].tag))\n                else:\n                    pre_dataset.append((*e, 'None'))\n\n#         random.seed(2021)\n        data = []\n        for e1, e2, tag in tqdm(pre_dataset):\n            tag1, tag2 = entity_tags.index(e1.tag), entity_tags.index(e2.tag)\n            e1, e2 = e1.text, e2.text\n            if (len(e1) > len(e2)):\n                text = e1.replace(e2, '[ ' + e2 + ' ]')\n            else:\n                text = e2.replace(e1, '[ ' + e1 + ' ]')\n            data.append((float(reversed_tags[tag]), tag1, tag2, *tok(text)))\n        \n        self.data = data\n    \n    def __getitem__(self, index):\n        return self.data[index]\n    \n    def __len__(self):\n        return len(self.data)\n    ","metadata":{"id":"wBfCFhgd6dB5","cellId":"6b9fb08d-f9a8-4462-89f9-f460cc6b72ec","trusted":true},"outputs":[],"execution_count":173},{"cell_type":"code","source":"#!g1.1\ntrain_data = MyDataset('NEREL/NEREL-v1.0/train')\ntrain_dl = DataLoader(train_data, shuffle=True, batch_size=32)\n\ndev_data = MyDataset('NEREL/NEREL-v1.0/dev')\ndev_dl = DataLoader(dev_data, batch_size=32)","metadata":{"outputId":"691101df-e6f5-4892-bcf9-095da0682d2e","id":"rvd8pCDp9nP_","cellId":"d307436a-2010-470f-9776-af9604b914d0","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":297,"referenced_widgets":["a4cfe2484121444aa63a6f186dd1702d","09c1193eb65d49ddbe17782b10b2a28d","d235f966faa6422d957e4663f531f702","3cc0beef069e4487b4787dbc86cfdc17","c00fff84efb4422ebae3f0660b04b024","506d596cf5d14cbc9ae5cdbca0eed41a","2ab1804bc4cb4403a45a819d176cf1e8","66ab50997c66473ebec11e720ceb10eb","98fcbf82905b45d582497d16b4fc891d","e1f100f2b91d4f48b6fa15c32d4eaa90","bf0e06dea8d54891aa7850733224fe6a","b35e1699140440a48100850aaf7cf8d0","c4958973a07d4e20b1a7e44c015521fa","6fa03b923c3e49229ce47fa7a129fe83","ba77f76f2f8640d1a5079a8f385ac199","19b49586c4f448d5b455ad80607877a4"]}},"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=745.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7325094bf0040a0a8cc8eadeed3f0b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11175.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b9c8644a4b7466382f1b283c2cc8145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=93.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6a07e27e5bd43d1a17d93c7f9a77f0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1301.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e783dee97ba44258817e5a00348b693"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"incorrect format in: row=По словам очевидцев пешехо\n file=21013_text\n\n\n\n\n"}],"execution_count":174},{"cell_type":"code","source":"#!g1.1\nnext(iter(dev_dl))","metadata":{"outputId":"d92ebfcb-b8aa-44db-e4af-9500b36b7dd6","id":"xxzNJH5Y_xtP","cellId":"43abbd3c-3534-4227-9254-9faca815eb87","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"[tensor([0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n         1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.],\n        dtype=torch.float64),\n tensor([ 3,  3,  3,  3,  6,  6,  6,  6,  6,  6,  3,  3,  5, 17, 27, 13,  8,  8,\n          6,  6,  6,  9, 22, 19,  3,  3,  3,  3,  1,  1,  1,  3]),\n tensor([ 3, 28,  3, 28,  6,  6,  6, 28,  6, 28, 28,  3,  6, 27, 17,  8,  3,  8,\n          8,  6, 28, 22, 22,  9,  6,  6,  3, 28, 18, 18, 18,  3]),\n tensor([[   101,    222,  17710,  ...,      0,      0,      0],\n         [   101,  17710,    876,  ...,      0,      0,      0],\n         [   101,    222,  13796,  ...,      0,      0,      0],\n         ...,\n         [   101, 113644,  12333,  ...,      0,      0,      0],\n         [   101, 113644,  12333,  ...,      0,      0,      0],\n         [   101,    222,  11615,  ...,      0,      0,      0]]),\n tensor([[1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         ...,\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0]]),\n tensor([[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0]])]"},"metadata":{}}],"execution_count":175},{"cell_type":"code","source":"#!g1.1\nnext(iter(dev_dl))[1].size()","metadata":{"outputId":"c29a7530-2400-499a-8736-79bcd56c8976","id":"8bymmZZDFlkk","cellId":"0d8e26a9-85e9-48e9-8f47-8cc686180488","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"torch.Size([32])"},"metadata":{}}],"execution_count":176},{"cell_type":"markdown","source":"# Training","metadata":{"cellId":"wlzglxz3q9f5e78w96x64x"}},{"cell_type":"code","source":"#!g1.1\n# from transformers import BertTokenizer, BertModel\n# import torch\n\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# model = BertModel.from_pretrained('bert-base-uncased')\n\n# inputs = tokenizer.encode_plus(\"Hello, my dog is cute\", return_tensors=\"pt\")\n# outputs = model(**inputs)\n\n# pooled_output = outputs[1]","metadata":{"cellId":"k1cex4rezebeuwvgyha3v","trusted":true},"outputs":[],"execution_count":177},{"cell_type":"code","source":"#!g1.1\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(\n            \"DeepPavlov/rubert-base-cased-sentence\", # Use the 12-layer BERT model, with an uncased vocab.\n            output_attentions = False, # Whether the model returns attentions weights.\n            output_hidden_states = False, # Whether the model returns all hidden-states.\n            max_length = MAX_LENGTH\n            )\n        self.dropout = nn.Dropout(0.1)\n        self.emb = nn.Embedding(29, 10)\n        self.fc = nn.Sequential(\n            nn.Linear(768 + 10*2, 100),\n            nn.LeakyReLU(True),\n            nn.Linear(100, 1)\n            )\n        \n    def forward(self, ent1, ent2, input_ids, token_type_ids, attention_mask):\n        ent1 = self.emb(ent1)\n        ent2 = self.emb(ent2)\n        out = self.bert(input_ids = input_ids, token_type_ids= token_type_ids, attention_mask = attention_mask)[1]\n        out = self.dropout(out)\n        out = torch.cat((out, ent1, ent2), dim=1)\n        out = self.fc(out)\n        return out","metadata":{"cellId":"ys9nkq9sz0ou2vklzyq29","trusted":true},"outputs":[],"execution_count":194},{"cell_type":"code","source":"#!g1.1\nmodel = MyModel()\nmodel.to(device)","metadata":{"scrolled":true,"cellId":"rdjndybvoq14kb16kazea","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"MyModel(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (emb): Embedding(29, 10)\n  (fc): Sequential(\n    (0): Linear(in_features=788, out_features=100, bias=True)\n    (1): LeakyReLU(negative_slope=True)\n    (2): Linear(in_features=100, out_features=1, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":195},{"cell_type":"code","source":"#!g1.1\n# model(torch.tensor([1]), torch.tensor([1]), **tokenizer.encode_plus(\"привет\", return_tensors=\"pt\"))","metadata":{"cellId":"3tp1x9dq37ige1ehscmbii","trusted":true},"outputs":[],"execution_count":181},{"cell_type":"code","source":"#!g1.1\ntokenizer.encode_plus(\"привет\", return_tensors=\"pt\")","metadata":{"cellId":"k5ceaxnxghmemhqp2fwn9w","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"{'input_ids': tensor([[  101, 26856,   102]]),\n 'token_type_ids': tensor([[0, 0, 0]]),\n 'attention_mask': tensor([[1, 1, 1]])}"},"metadata":{}}],"execution_count":182},{"cell_type":"code","source":"#!g1.1\ndef calculate_accuracy(y_pred, y):\n    return sum(torch.round(torch.sigmoid(y_pred)) == y)*1.0/len(y)\n\ndef train():\n    model.train()\n    running_loss = 0\n    epoch_accuracy = 0\n\n    pbar = tqdm(enumerate(train_dl), total = len(train_dl))\n    for i, (labels, tag1, tag2, input_ids, attention_mask, token_type_ids) in pbar:\n        labels = labels.to(device)\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        token_type_ids = token_type_ids.to(device)\n        tag1 = tag1.to(device)\n        tag2 = tag2.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(\n                    tag1,\n                    tag2,\n                    input_ids = input_ids, \n                    token_type_ids=token_type_ids,\n                    attention_mask=attention_mask)\n        \n#         print(outputs.shape, labels.shape)\n        outputs = outputs.view(-1)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n#         print(outputs, labels)\n        epoch_accuracy += calculate_accuracy(outputs, labels).item()\n        running_loss += loss.item()\n        pbar.set_description(f'training: running_loss = {running_loss/(i+1.0):.4f} accuracy = {epoch_accuracy/(i+1.0):.4f},')\n    \n    scheduler.step(running_loss/(i+1))\n    print(f'train loss= {running_loss/(i+1):.4f} \\n train accuracy = {epoch_accuracy/(i+1):.4f},')\n\n    \nfrom sklearn.metrics import f1_score\ndef test():\n    with torch.no_grad():\n        model.eval()\n        running_loss = 0\n        epoch_accuracy = 0\n\n        dev_dl = DataLoader(dev_data, batch_size=32)\n\n        pbar = tqdm(enumerate(dev_dl), total = len(dev_dl))\n\n        y_true = []\n        y_pred = []\n\n        for i, (labels, tag1, tag2, input_ids, attention_mask, token_type_ids) in pbar:\n            labels = labels.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            token_type_ids = token_type_ids.to(device)\n            tag1 = tag1.to(device)\n            tag2 = tag2.to(device)\n            \n            outputs = model(\n                    tag1,\n                    tag2,\n                    input_ids = input_ids, \n                    token_type_ids=token_type_ids,\n                    attention_mask=attention_mask)\n            \n            \n            outputs = outputs.view(-1)\n            loss = criterion(outputs, labels)\n#             calculate_accuracy(outputs[1], labels)\n            \n            y_true += [e.item() for e in labels]\n            y_pred += [e.item() for e in torch.round(torch.sigmoid(outputs))]\n\n            running_loss += loss.item()\n            epoch_accuracy += calculate_accuracy(outputs, labels).item()\n            # pbar.set_description(f'testing: running_loss = {running_loss/(i+1):.4f} accuracy = {epoch_accuracy/(i+1):.4f},')\n    print(f'test loss= {running_loss/(i+1):.4f} \\n test accuracy = {epoch_accuracy/(i+1):.4f}, F1 = {f1_score(y_true, y_pred)}')","metadata":{"id":"9jmrqyEk_MTw","cellId":"d470c70e-ba8e-44f1-91d0-65c80334cbda","trusted":true},"outputs":[],"execution_count":192},{"cell_type":"code","source":"#!g1.1\noptimizer = torch.optim.AdamW(model.parameters(),\n                  lr = 5e-5, # args.learning_rate \n                  eps = 5e-8#1e-8 # args.adam_epsilon \n                )\ncriterion = nn.BCEWithLogitsLoss()\nscheduler = ReduceLROnPlateau(optimizer, patience=5, cooldown = 1, factor = 0.5)","metadata":{"id":"ggQK1n44_3E_","cellId":"6075ac4b-e8b4-45f5-9054-cf98dbd8964b","trusted":true},"outputs":[],"execution_count":196},{"cell_type":"code","source":"#!g1.1\nseed_val = 2021\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\nfor epoch in range(1, 21):\n    print('lr=', optimizer.param_groups[0]['lr'])\n    print(f'epoch = {epoch}')\n    train()\n    test()","metadata":{"outputId":"0ece41b4-a99d-4cbd-caff-7d1dbaa4c998","id":"k-BXDd2F-gOv","cellId":"596d3a26-d2df-4534-8083-05b39a785275","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":487,"referenced_widgets":["b60e883030494014ad492832403b33dd","5fc641cfc31d4b55accf2b2e4d9174ed","84b6979101ac48c487a026a1ac83f08c","a310f204604d4dbf8a88a45900d0a9c7","744627484db14fe68e41a9f7b0db3a91","1d80c7709b284caf9d2c5fc8dd8db5f8","5cedf898058e4a969a188e9b4a8b5093","1d28a69fd87c4549bd491c270c3b2335","ca264fe88e414dfd99c96b811722f296","29d4d96d29e044808d88ba19e63f81d3","9a9b5effe3c048069641829df3495a1e","2dd5e1489bd847b29b4148c68352e37a","7364869ba6d94b80ac309d59a47362d1"]}},"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"448ff9da2bf94e9d94d5a88aa6b6bb4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7358421a1bb34233addaf6b9c86cddf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"159b297a6b564e0eb25808f795a41be1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aebd5ab38504836bc223c2e377cbc72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9c8a673996f4328a263c6c0bc8e5106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be504a0caa334fe4afbda93477468aff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23128cc957334ee4866d26aac9cbef0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b5f16a714b94fc3b36dd459255ff2ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f344d32ff61420688f3e8554df70a9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2186acf5eeb459e86bcbcb8aaf0961e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bec573b3b4046909a2f7885443db8d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f1e41e714f469688570c7b3c8f0bf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c54fa9fa92654a6f9ae8ef92d76bec0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4365c18e0b14123aee994d3cb81cfda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a4585c6624c47058587a626d99ddcdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"992b1a12feb3461eaae8d6d4dfb2ef00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6304c661a2c4e7c94c10ec5cc615c23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"245ce4f6581944508b090eee9c671c49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ce5b0e967a455591c0cc4cdd6afbe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25e0b45f3b9a4ad783ac56a0deff8a93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7873950452d94711a9fed049b7f93572"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a92a73f80324a0882b91a14e665f869"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea9ac7d430d54110b70d9c13db412a69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0ab602e01b47c2841fd9a71397bb5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7720352579d497692632660d81f3b0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aba9559d499415bbea4c23df400a1c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e1abaac30104fad8eaa239283af3c7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eff8538e0fbc4e7cbc5116c7a9fb1672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aed08e0ee6349be88de73fa77d2293f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ba04e0cf39545bdb43142088dcb1e28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa3f575218484f9b89b209b7a2d45f0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d44a2a759e2c4d1a8b945e72db8399d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d82713260bd348dcaa52f746250f3300"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5a2cb5e0aba493c93d7cd57e91470a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eba2d6395d2e4114b082a9064b3dc75a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d02bd8eddd224ed38a580beb5360a3b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3b8304229b44b728279492c0da6b3cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c81bd794c9f041cdb4e31ab5638eaa19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"618e6288327d42bfa9f76ca51b69ab96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"784441b17099421da3cf3923e645179c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"lr= 5e-05\nepoch = 1\n\ntrain loss= 0.3528 \n train accuracy = 0.8516,\n\ntest loss= 0.3484 \n test accuracy = 0.8563, F1 = 0.8234183191690273\nlr= 5e-05\nepoch = 2\n\ntrain loss= 0.2573 \n train accuracy = 0.9107,\n\ntest loss= 0.3996 \n test accuracy = 0.8563, F1 = 0.8074150360453141\nlr= 5e-05\nepoch = 3\n\ntrain loss= 0.2328 \n train accuracy = 0.9234,\n\ntest loss= 0.3717 \n test accuracy = 0.8612, F1 = 0.8132780082987551\nlr= 5e-05\nepoch = 4\n\ntrain loss= 0.1914 \n train accuracy = 0.9399,\n\ntest loss= 0.4320 \n test accuracy = 0.8345, F1 = 0.760845383759733\nlr= 5e-05\nepoch = 5\n\ntrain loss= 0.1801 \n train accuracy = 0.9431,\n\ntest loss= 0.3491 \n test accuracy = 0.8753, F1 = 0.8430232558139535\nlr= 5e-05\nepoch = 6\n\ntrain loss= 0.1657 \n train accuracy = 0.9487,\n\ntest loss= 0.4612 \n test accuracy = 0.8643, F1 = 0.8297872340425532\nlr= 5e-05\nepoch = 7\n\ntrain loss= 0.1486 \n train accuracy = 0.9555,\n\ntest loss= 0.4131 \n test accuracy = 0.8723, F1 = 0.8316430020283976\nlr= 5e-05\nepoch = 8\n\ntrain loss= 0.1398 \n train accuracy = 0.9581,\n\ntest loss= 0.4752 \n test accuracy = 0.8669, F1 = 0.830558276199804\nlr= 5e-05\nepoch = 9\n\ntrain loss= 0.1398 \n train accuracy = 0.9589,\n\ntest loss= 0.4381 \n test accuracy = 0.8738, F1 = 0.8326530612244898\nlr= 5e-05\nepoch = 10\n\ntrain loss= 0.1234 \n train accuracy = 0.9653,\n\ntest loss= 0.4169 \n test accuracy = 0.8528, F1 = 0.8136585365853659\nlr= 5e-05\nepoch = 11\n\ntrain loss= 0.1223 \n train accuracy = 0.9666,\n\ntest loss= 0.5428 \n test accuracy = 0.8597, F1 = 0.8169014084507042\nlr= 5e-05\nepoch = 12\n\ntrain loss= 0.1200 \n train accuracy = 0.9670,\n\ntest loss= 0.6092 \n test accuracy = 0.8540, F1 = 0.8020833333333334\nlr= 5e-05\nepoch = 13\n\ntrain loss= 0.1171 \n train accuracy = 0.9691,\n\ntest loss= 0.6042 \n test accuracy = 0.8379, F1 = 0.8104693140794225\nlr= 5e-05\nepoch = 14\n\ntrain loss= 0.1148 \n train accuracy = 0.9705,\n\ntest loss= 0.5314 \n test accuracy = 0.8540, F1 = 0.8053278688524591\nlr= 5e-05\nepoch = 15\n\ntrain loss= 0.1487 \n train accuracy = 0.9612,\n\ntest loss= 0.4979 \n test accuracy = 0.8502, F1 = 0.8044132397191575\nlr= 5e-05\nepoch = 16\n\ntrain loss= 0.1587 \n train accuracy = 0.9579,\n\ntest loss= 0.5389 \n test accuracy = 0.8433, F1 = 0.8068181818181818\nlr= 5e-05\nepoch = 17\n\ntrain loss= 0.1833 \n train accuracy = 0.9483,\n\ntest loss= 0.4914 \n test accuracy = 0.8563, F1 = 0.8131868131868131\nlr= 5e-05\nepoch = 18\n\ntrain loss= 0.2855 \n train accuracy = 0.8809,\n\ntest loss= 0.4939 \n test accuracy = 0.7697, F1 = 0.7516556291390728\nlr= 5e-05\nepoch = 19\n\ntrain loss= 0.2613 \n train accuracy = 0.9108,\n\ntest loss= 0.4949 \n test accuracy = 0.8464, F1 = 0.7890295358649788\nlr= 5e-05\nepoch = 20\n\ntrain loss= 0.1792 \n train accuracy = 0.9521,\n\ntest loss= 0.5064 \n test accuracy = 0.8414, F1 = 0.7742639040348964\n"}],"execution_count":197},{"cell_type":"code","source":"#!g1.1\ntorch.save(model.state_dict(), 'models/Bert_binary_nested_only_ent_tags')","metadata":{"id":"EsPbh2mbK9lo","cellId":"f6afc164-beb3-44a6-8c30-394056f63d42","trusted":true},"outputs":[],"execution_count":198},{"cell_type":"code","source":"# #!g1.1\n# # from sklearn.metrics import f1_score\n\n# with torch.no_grad():\n#     model.eval()\n#     running_loss = 0\n#     epoch_accuracy = 0\n\n#     dev_dl = DataLoader(dev_data, batch_size=1)\n\n#     pbar = tqdm(enumerate(dev_dl), total = len(dev_dl))\n\n#     y_true = []\n#     y_pred = []\n\n#     for i, (labels, input_ids, attention_mask, token_type_ids) in pbar:\n#         labels = labels.to(device)\n#         input_ids = input_ids.to(device)\n#         attention_mask = attention_mask.to(device)\n#         token_type_ids = token_type_ids.to(device)\n\n#         outputs = model(input_ids, \n#                     token_type_ids=token_type_ids,\n#                     attention_mask=attention_mask, \n#                     labels=labels)\n\n\n#         loss = outputs[0]\n# #         print(input_ids[0].detach().cpu().numpy())\n#         if (torch.argmax(outputs[1], axis = 1) != labels):\n#             print('expected:', labels.detach().cpu().item())\n#             print('found:', torch.argmax(outputs[1], axis = 1).detach().cpu().item(), '(', outputs[1].detach().cpu().numpy() ,')')\n#             print(tokenizer.decode(input_ids[0].detach().cpu().numpy()).replace(r'[PAD]', ''))","metadata":{"cellId":"zdwbq1krg3hb4q9nmle1g","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n\nwith torch.no_grad():\n    model.eval()\n    running_loss = 0\n    epoch_accuracy = 0\n\n    dev_dl = DataLoader(dev_data, batch_size=1)\n\n    pbar = tqdm(enumerate(dev_dl), total = len(dev_dl))\n\n    y_true = []\n    y_pred = []\n    tags = []\n\n    for i, (labels, tag1, tag2, input_ids, attention_mask, token_type_ids) in pbar:\n        labels = labels.to(device)\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        token_type_ids = token_type_ids.to(device)\n        tag1 = tag1.to(device)\n        tag2 = tag2.to(device)\n\n        outputs = model(\n                tag1,\n                tag2,\n                input_ids = input_ids, \n                token_type_ids=token_type_ids,\n                attention_mask=attention_mask)\n\n\n        outputs = outputs.view(-1)\n        loss = criterion(outputs, labels)\n#         calculate_accuracy(outputs[1], labels)\n\n        y_true += [e.item() for e in labels]\n        y_pred += [e.item() for e in torch.round(torch.sigmoid(outputs))]\n        tags += [(tag1, tag2)]\n        \n        running_loss += loss.item()\n        epoch_accuracy += calculate_accuracy(outputs, labels).item()\n        # pbar.set_description(f'testing: running_loss = {running_loss/(i+1):.4f} accuracy = {epoch_accuracy/(i+1):.4f},')\nprint(f'test loss= {running_loss/(i+1):.4f} \\n test accuracy = {epoch_accuracy/(i+1):.4f}, F1 = {f1_score(y_true, y_pred)}')","metadata":{"cellId":"6xfib7qz66g002ib3a7g1o1y","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1301.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a128eacb89440cb93b032f2551d154"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"\ntest loss= 0.5078 \n test accuracy = 0.8409, F1 = 0.7742639040348964\n"},{"output_type":"stream","name":"stderr","text":"/kernel/lib/python3.7/site-packages/ml_kernel/kernel.py:859: UserWarning: The following variables cannot be serialized: pbar\n  warnings.warn(message)\n"}],"execution_count":200},{"cell_type":"code","source":"#!g1.1\ndef fun(e):\n    return entity_tags[e[0].item()], entity_tags[e[1].item()]\nl = []\nfor true, pred, t in zip(y_true, y_pred, tags):\n    if (true != pred):\n        l.append(fun(t))","metadata":{"cellId":"vt8uf2id1bflwmqwxqy5ti","trusted":true},"outputs":[],"execution_count":209},{"cell_type":"code","source":"#!g1.1\nfrom collections import Counter\nCounter(l).most_common()","metadata":{"scrolled":true,"cellId":"r5qz3qbcyvrcipel355imh","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"[(('PROFESSION', 'ORGANIZATION'), 33),\n (('ORGANIZATION', 'ORGANIZATION'), 18),\n (('EVENT', 'LOCATION'), 10),\n (('ORGANIZATION', 'STATE_OR_PROVINCE'), 8),\n (('ORGANIZATION', 'COUNTRY'), 8),\n (('PROFESSION', 'COUNTRY'), 7),\n (('EVENT', 'EVENT'), 6),\n (('PROFESSION', 'STATE_OR_PROVINCE'), 6),\n (('DISEASE', 'DISEASE'), 5),\n (('EVENT', 'PERSON'), 5),\n (('PERSON', 'ORGANIZATION'), 4),\n (('LAW', 'LAW'), 4),\n (('PROFESSION', 'PROFESSION'), 4),\n (('PERSON', 'EVENT'), 4),\n (('ORGANIZATION', 'CITY'), 4),\n (('PROFESSION', 'LOCATION'), 3),\n (('EVENT', 'DISEASE'), 3),\n (('LAW', 'STATE_OR_PROVINCE'), 3),\n (('ORGANIZATION', 'RELIGION'), 3),\n (('WORK_OF_ART', 'PERSON'), 3),\n (('EVENT', 'CITY'), 3),\n (('EVENT', 'STATE_OR_PROVINCE'), 3),\n (('LOCATION', 'ORGANIZATION'), 2),\n (('PERSON', 'FAMILY'), 2),\n (('LAW', 'COUNTRY'), 2),\n (('PENALTY', 'MONEY'), 2),\n (('PROFESSION', 'EVENT'), 2),\n (('PROFESSION', 'CITY'), 2),\n (('ORGANIZATION', 'PROFESSION'), 2),\n (('ORGANIZATION', 'DISTRICT'), 2),\n (('PERSON', 'FACILITY'), 2),\n (('EVENT', 'WORK_OF_ART'), 2),\n (('PROFESSION', 'PERSON'), 2),\n (('ORGANIZATION', 'LOCATION'), 2),\n (('FACILITY', 'LOCATION'), 2),\n (('EVENT', 'COUNTRY'), 2),\n (('FACILITY', 'PERSON'), 2),\n (('FACILITY', 'ORGANIZATION'), 2),\n (('AWARD', 'COUNTRY'), 2),\n (('COUNTRY', 'WORK_OF_ART'), 2),\n (('COUNTRY', 'EVENT'), 1),\n (('ORGANIZATION', 'LAW'), 1),\n (('LAW', 'DATE'), 1),\n (('ORGANIZATION', 'PRODUCT'), 1),\n (('COUNTRY', 'PROFESSION'), 1),\n (('LAW', 'CRIME'), 1),\n (('FACILITY', 'DISTRICT'), 1),\n (('ORGANIZATION', 'NUMBER'), 1),\n (('AWARD', 'EVENT'), 1),\n (('LOCATION', 'WORK_OF_ART'), 1),\n (('PERSON', 'WORK_OF_ART'), 1),\n (('COUNTRY', 'ORGANIZATION'), 1),\n (('IDEOLOGY', 'ORGANIZATION'), 1),\n (('PROFESSION', 'FACILITY'), 1),\n (('AWARD', 'DATE'), 1),\n (('EVENT', 'ORGANIZATION'), 1),\n (('ORGANIZATION', 'FACILITY'), 1),\n (('PERSON', 'COUNTRY'), 1),\n (('LOCATION', 'CITY'), 1),\n (('STATE_OR_PROVINCE', 'STATE_OR_PROVINCE'), 1),\n (('PROFESSION', 'IDEOLOGY'), 1),\n (('NATIONALITY', 'ORGANIZATION'), 1),\n (('ORGANIZATION', 'NATIONALITY'), 1),\n (('LOCATION', 'COUNTRY'), 1)]"},"metadata":{}}],"execution_count":215},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"t8s50aqic5m0moeino3p"},"outputs":[],"execution_count":null}]}